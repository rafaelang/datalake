{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conding: utf-8\n",
    "\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from sympy.interactive import printing\n",
    "printing.init_printing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Este trecho de código é necessário apenas quando o notebook é executado em ambiente local\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "try:\n",
    "    sc = SparkContext('local')\n",
    "    spark = SparkSession(sc)\n",
    "except ValueError:\n",
    "    pass  # a spark context is already created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recuperando schema interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structured_jsons_path = '../../../../aws_s3/teste/structured_json/checkout/00_CheckoutOrder/001E600A63944702BF4EA7A92FCD3833/id/*'\n",
    "structured_jsons_path = '../../../../data/s3/checkout/structured_json/{*}CheckoutOrder/*/id/*'\n",
    "structured_df = spark.read.json(structured_jsons_path)\n",
    "\n",
    "schema = structured_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lendo dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_datapath = '../../../../aws_s3/teste/history/checkout/001E600A63944702BF4EA7A92FCD3833/id/*'\n",
    "history_datapath = '../../../../data/s3/checkout/history/YY_CheckoutOrder/*/id/*'\n",
    "df = spark.read.json(history_datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Ao todo há {} observações nos dados\".format(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "def has_column(df, col):\n",
    "    try:\n",
    "        df[col]\n",
    "        return True\n",
    "    except AnalysisException:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código para converter os dados com spark usando cast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Casting de Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Item Type\n",
    "types = filter(lambda f: f.name == \"Items\", structured_df.schema.fields)\n",
    "itemType = types[0].dataType\n",
    "\n",
    "def convert_item(data):\n",
    "    def _parse_product_categories(raw_product_categories):\n",
    "        product_categories = []\n",
    "        for k,v in raw_product_categories.items():\n",
    "            product_categories.append({\n",
    "                'id': k,\n",
    "                'name': v\n",
    "            })\n",
    "        return product_categories\n",
    "    \n",
    "    items = json.loads(data)\n",
    "    for item in items:\n",
    "        product_categories = _parse_product_categories(item.get('productCategories', {}))\n",
    "        item['productCategories'] = product_categories\n",
    "\n",
    "    return items\n",
    "\n",
    "# Register functions as Spark UDFs \n",
    "udf_getData = UserDefinedFunction(convert_item, itemType)\n",
    "\n",
    "df = df.withColumn('Items', udf_getData(\"Items\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "\n",
    "df.select(\"Items\").show(5)\n",
    "\n",
    "e = time.time()\n",
    "\n",
    "print \"Tempo executando: {}\".format(e-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Casting de ItemMetadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = filter(lambda f: f.name == \"ItemMetadata\", structured_df.schema.fields)\n",
    "item_metadata_type = types[0].dataType\n",
    "\n",
    "def convert_item_metadata(data):\n",
    "    if data:\n",
    "        itemmetadata = json.loads(data)\n",
    "        len_items_itemmetadata = len(itemmetadata[\"items\"])\n",
    "        for i in range(len_items_itemmetadata):\n",
    "            if (\"assemblyOptions\" in itemmetadata[\"items\"][i]):\n",
    "                del itemmetadata[\"items\"][i][\"assemblyOptions\"]\n",
    "        return itemmetadata\n",
    "        \n",
    "# Register functions as Spark UDFs \n",
    "udf_convert_item_metadata = UserDefinedFunction(convert_item_metadata, item_metadata_type)\n",
    "\n",
    "if has_column(df, \"ItemMetadata\"):\n",
    "    df = df.withColumn(\"ItemMetadata\", udf_convert_item_metadata(\"ItemMetadata\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "\n",
    "df.select(\"ItemMetadata\").show(5)\n",
    "\n",
    "e = time.time()\n",
    "\n",
    "print \"Tempo executando: {}\".format(e-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Casting RateAndBenefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = filter(lambda f: f.name == \"RatesAndBenefitsData\", structured_df.schema.fields)\n",
    "rateandbenefits_type = types[0].dataType\n",
    "\n",
    "def convert_ratesandbenefits(data):\n",
    "    KEY_IDENTIFIERS = \"rateAndBenefitsIdentifiers\"\n",
    "    KEY_MATCH_PARAMS = \"matchedParameters\"\n",
    "    KEY_ADDINFO = \"additionalInfo\"\n",
    "    data = json.loads(data)\n",
    "    if data and KEY_IDENTIFIERS in data:\n",
    "        for i in range(len(data[KEY_IDENTIFIERS])):\n",
    "            if KEY_MATCH_PARAMS in data[KEY_IDENTIFIERS][i]:\n",
    "                del data[KEY_IDENTIFIERS][i][KEY_MATCH_PARAMS]\n",
    "            if KEY_ADDINFO in data[KEY_IDENTIFIERS][i]:\n",
    "                del data[KEY_IDENTIFIERS][i][KEY_ADDINFO]\n",
    "    return data\n",
    "        \n",
    "# Register functions as Spark UDFs \n",
    "udf_convert_ratesandbenefits = UserDefinedFunction(convert_ratesandbenefits, rateandbenefits_type)\n",
    "\n",
    "if has_column(df, \"RatesAndBenefitsData\"):\n",
    "    df = df.withColumn(\"RatesAndBenefitsData\", udf_convert_ratesandbenefits(\"RatesAndBenefitsData\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "\n",
    "df.select(\"RatesAndBenefitsData\").show(1, False)\n",
    "\n",
    "e = time.time()\n",
    "\n",
    "print \"Tempo executando: {}\".format(e-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Casting CustomData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.utils import AnalysisException\n",
    "try:\n",
    "    print df.where(\"CustomData is not null\").select(\"CustomData\").count()\n",
    "except AnalysisException:\n",
    "    print \"There is no col CustomData in data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = filter(lambda f: f.name == \"CustomData\", structured_df.schema.fields)\n",
    "customdata_type = types[0].dataType\n",
    "\n",
    "def convert_customdata(data):\n",
    "    KEY_CUSTOMAPP = \"customApps\"\n",
    "    KEY_FIELDS = \"fields\"\n",
    "    KEY_EXTRA_CONTENT = \"cart-extra-context\"\n",
    "    \n",
    "    customdata = data and json.loads(data)\n",
    "    if customdata and KEY_CUSTOMAPP in customdata:\n",
    "        for i in range(len(customdata[KEY_CUSTOMAPP])):\n",
    "            if KEY_FIELDS in customdata[KEY_CUSTOMAPP][i] and\\\n",
    "                KEY_EXTRA_CONTENT in customdata[KEY_CUSTOMAPP][i][KEY_FIELDS]:\n",
    "                    del customdata[KEY_CUSTOMAPP][i][KEY_FIELDS][KEY_EXTRA_CONTENT]\n",
    "    return customdata\n",
    "        \n",
    "# Register functions as Spark UDFs \n",
    "udf_convert_customdata = UserDefinedFunction(convert_customdata, customdata_type)\n",
    "\n",
    "\n",
    "if has_column(df, \"CustomData\"):\n",
    "    df = df.withColumn(\"CustomData\", udf_convert_customdata(\"CustomData\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "\n",
    "df.select(\"CustomData\").show(5)\n",
    "\n",
    "e = time.time()\n",
    "\n",
    "print \"Tempo executando: {}\".format(e-s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Casting Attachment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTACHMENT = \"attachment\"\n",
    "\n",
    "\n",
    "def remove_attachments(dic):    \n",
    "    dic_copy = dic.copy()\n",
    "    for key in dic_copy:\n",
    "        if(ATTACHMENT in key.lower()):\n",
    "            del dic[key]\n",
    "        elif(type(dic_copy[key]) == dict):\n",
    "            remove_attachments(dic[key])\n",
    "        elif(type(dic_copy[key]) == list):\n",
    "            for item in dic_copy[key]:\n",
    "                if(type(item) == dict):\n",
    "                    remove_attachments(item)\n",
    "    return dic\n",
    "    \n",
    "def field_cleansing(field, cleansing_func=remove_attachments):\n",
    "    if field is not str: return\n",
    "    structuted_field = field and json.loads(field)\n",
    "    if type(structuted_field) is dict:\n",
    "        structuted_field = cleansing_func(structuted_field)\n",
    "    return structuted_field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for field in df.schema.fields:\n",
    "    field_type = field.dataType\n",
    "    field_name = field.name\n",
    "\n",
    "    if ATTACHMENT in field_name.lower():\n",
    "        df = df.drop(field_name)\n",
    "    elif field_type != StringType():\n",
    "        udf_get_transform_data = UserDefinedFunction(lambda f: field_cleansing(f), field_type)\n",
    "        df = df.withColumn(field_name, udf_get_transform_data(field_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = time.time()\n",
    "\n",
    "df.show(1, False)\n",
    "\n",
    "e = time.time()\n",
    "\n",
    "print \"Tempo executando: {}\".format(e-s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Particionando dados e escrevendo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getYear(lastChange, creationDate):\n",
    "    date = lastChange if lastChange is not None else creationDate\n",
    "    return date.split('T')[0].split('-')[0]\n",
    "\n",
    "def getMonth(lastChange, creationDate):\n",
    "    date = lastChange if lastChange is not None else creationDate\n",
    "    return date.split('T')[0].split('-')[1]\n",
    "\n",
    "def getDay(lastChange, creationDate):\n",
    "    date = lastChange if lastChange is not None else creationDate\n",
    "    return date.split('T')[0].split('-')[2]\n",
    "\n",
    "# Register functions as Spark UDFs \n",
    "udf_getYear = UserDefinedFunction(getYear, StringType())\n",
    "udf_getMonth = UserDefinedFunction(getMonth, StringType())\n",
    "udf_getDay = UserDefinedFunction(getDay, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Columns for the Partitions\n",
    "df = df.withColumn('YEAR', udf_getYear(df.LastChange, df.CreationDate))\n",
    "df = df.withColumn('MONTH', udf_getMonth(df.LastChange, df.CreationDate))\n",
    "df = df.withColumn('DAY', udf_getDay(df.LastChange, df.CreationDate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save table to S3 using Parquet format and partitioning by defined columns\n",
    "df.write.partitionBy(['YEAR','MONTH','DAY','InstanceId'])\\\n",
    "    .mode('append')\\\n",
    "    .parquet('../../../../blablabla/')\n",
    "#     .parquet('s3://vtex.datalake/consumable_tables/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
