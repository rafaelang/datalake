{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conding: utf-8\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.functions import unix_timestamp\n",
    "from pyspark.sql.functions import UserDefinedFunction \n",
    "from pyspark.sql.functions import struct\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "import boto3\n",
    "\n",
    "import os\n",
    "import json\n",
    "import urllib\n",
    "import re\n",
    "from unicodedata import normalize\n",
    "from itertools import islice\n",
    "\n",
    "from gen_struct import transform_struct_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Este trecho de código é necessário apenas quando o notebook é executado em ambiente local de test\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "try:\n",
    "    sc = SparkContext('local')\n",
    "    spark = SparkSession(sc)\n",
    "except ValueError:\n",
    "    pass  # a spark context is already created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lendo objetos do S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_resources = boto3.resource('s3')\n",
    "\n",
    "bucket = filter(lambda bckt: bckt.name == 'vtex-analytics-import', s3_resources.buckets.all())[0]\n",
    "prefix = 'vtex-checkout-versioned/95_FulfillmentOrder/001E600A63944702BF4EA7A92FCD3833/'\n",
    "\n",
    "hexdir_paths = bucket.objects.filter(Prefix=prefix)\n",
    "ids_paths = filter(lambda obj: '/id/' in obj.key, hexdir_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estruturando objetos e criando dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "\n",
    "structured_jsons = []\n",
    "for obj in ids_paths:\n",
    "    response = s3_client.get_object(\n",
    "        Bucket=obj.bucket_name,\n",
    "        Key=obj.key,\n",
    "    )\n",
    "    raw_json = json.loads(response['Body'].read())\n",
    "    structured = transform_struct_json(raw_json)\n",
    "    structured_json = json.dumps(structured)\n",
    "    structured_jsons.append(structured_json)\n",
    "\n",
    "jsonRDD = sc.parallelize(structured_jsons)\n",
    "df = spark.read.json(jsonRDD)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Particionando dados e escrevendo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getYear(var):\n",
    "    return var.split('T')[0].split('-')[0]\n",
    "\n",
    "def getMonth(var):\n",
    "    return var.split('T')[0].split('-')[1]\n",
    "\n",
    "def getDay(var):\n",
    "    return var.split('T')[0].split('-')[2]\n",
    "\n",
    "# Register functions as Spark UDFs \n",
    "udf_getYear = UserDefinedFunction(getYear, StringType())\n",
    "udf_getMonth = UserDefinedFunction(getMonth, StringType())\n",
    "udf_getDay = UserDefinedFunction(getDay, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Columns for the Partitions\n",
    "df = df.withColumn('YEAR', udf_getYear(df.CreationDate))\n",
    "df = df.withColumn('MONTH', udf_getMonth(df.CreationDate))\n",
    "df = df.withColumn('DAY', udf_getDay(df.CreationDate))\n",
    "\n",
    "# Save table to S3 using Parquet format and partitioning by defined columns\n",
    "df.write.partitionBy(['YEAR','MONTH','DAY','InstanceId'])\\\n",
    "    .mode('append')\\\n",
    "    .json('../../../../data/s3/checkout/partitioned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdf = spark.read.json('../../../../data/s3/checkout/structured_json/{*}CheckoutOrder/*/id/*')\n",
    "# schema = stdf.schema\n",
    "# tdf = spark.read.json('../../../../data/s3/checkout/history/{*}CheckoutOrder/*/id/*', schema=schema)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
