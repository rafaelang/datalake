{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conding: utf-8\n",
    "\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Este trecho de código é necessário apenas quando o notebook é executado em ambiente local\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "try:\n",
    "    sc = SparkContext('local')\n",
    "    spark = SparkSession(sc)\n",
    "except ValueError:\n",
    "    pass  # a spark context is already created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recuperando schema interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_jsons_path = '../../../../aws_s3/teste/structured_json/checkout/00_CheckoutOrder/001E600A63944702BF4EA7A92FCD3833/id/*'\n",
    "structured_df = spark.read.json(structured_jsons_path)\n",
    "\n",
    "schema = structured_df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usando o schema para, ao ler dados históricos, convertê-los já para a estrutura ideal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Get Item Type\\ntypes = filter(lambda f: f.name == \"Items\", structured_df.schema.fields)\\nitemType = types[0].dataType\\n\\ndef convertData(dado):\\n    def _parse_product_categories(raw_product_categories):\\n        product_categories = []\\n        for k,v in raw_product_categories.items():\\n            product_categories.append({\\n                \\'id\\': k,\\n                \\'name\\': v\\n            })\\n        return product_categories\\n    \\n    if(dado == \"Items\"):\\n        items = json.loads(dado)\\n        for item in items:\\n            product_categories = _parse_product_categories(item.get(\\'productCategories\\', {}))\\n            item[\\'productCategories\\'] = product_categories\\n\\n    return items\\n\\n# Register functions as Spark UDFs \\nudf_getData = UserDefinedFunction(convertData, itemType)\\n\\ndf = df.withColumn(\\'Items\\', udf_getData(\"Items\"))\\n\\ndf.select(\"Items\").show()\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_datapath = '../../../../aws_s3/teste/history/checkout/001E600A63944702BF4EA7A92FCD3833//id/*'\n",
    "df = spark.read.json(history_datapath)\n",
    "\n",
    "# O código abaixo funciona para Items\n",
    "\n",
    "'''\n",
    "# Get Item Type\n",
    "types = filter(lambda f: f.name == \"Items\", structured_df.schema.fields)\n",
    "itemType = types[0].dataType\n",
    "\n",
    "def convertData(dado):\n",
    "    def _parse_product_categories(raw_product_categories):\n",
    "        product_categories = []\n",
    "        for k,v in raw_product_categories.items():\n",
    "            product_categories.append({\n",
    "                'id': k,\n",
    "                'name': v\n",
    "            })\n",
    "        return product_categories\n",
    "    \n",
    "    if(dado == \"Items\"):\n",
    "        items = json.loads(dado)\n",
    "        for item in items:\n",
    "            product_categories = _parse_product_categories(item.get('productCategories', {}))\n",
    "            item['productCategories'] = product_categories\n",
    "\n",
    "    return items\n",
    "\n",
    "# Register functions as Spark UDFs \n",
    "udf_getData = UserDefinedFunction(convertData, itemType)\n",
    "\n",
    "df = df.withColumn('Items', udf_getData(\"Items\"))\n",
    "\n",
    "df.select(\"Items\").show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Código para converter os dados com spark usando cast\n",
    "\n",
    "#### TODO: ver a forma de lidar com as comparações em `get_transform_data`, pois o field vem como uma coluna, teria de ver uma forma de pegar seu nome, para poder comparar e fazer as conversões corretas\n",
    "#### TODO: ver se o `remove_attachments` funciona corretamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "ATTACHMENT = \"attachment\"\n",
    "JSON_KEY_ITEMMETADATA = \"ItemMetadata\"\n",
    "JSON_KEY_ITEMS = \"Items\"\n",
    "JSON_KEY_RATESANDBDATA = \"RatesAndBenefitsData\"\n",
    "JSON_KEY_CUSTOMDATA = \"CustomData\"\n",
    "\n",
    "\n",
    "def remove_attachments(dic):    \n",
    "    dic_copy = dic.copy()\n",
    "    for key in dic_copy:\n",
    "        if(ATTACHMENT in key.lower()):\n",
    "            del dic[key]\n",
    "        elif(type(dic_copy[key]) == dict):\n",
    "            remove_attachments(dic[key])\n",
    "        elif(type(dic_copy[key]) == list):\n",
    "            for item in dic_copy[key]:\n",
    "                if(type(item) == dict):\n",
    "                    remove_attachments(item)\n",
    "    return dic\n",
    "\n",
    "\n",
    "def parse_items(items):\n",
    "    def _parse_product_categories(raw_product_categories):\n",
    "        product_categories = []\n",
    "        for k,v in raw_product_categories.items():\n",
    "            product_categories.append({\n",
    "                'id': k,\n",
    "                'name': v\n",
    "            })\n",
    "        return product_categories\n",
    "    \n",
    "    for item in items:\n",
    "        product_categories = _parse_product_categories(item.get('productCategories', {}))\n",
    "        item['productCategories'] = product_categories\n",
    "        \n",
    "    return items\n",
    "\n",
    "\n",
    "def transform_itemmetadata(itemmetadata):\n",
    "    len_items_itemmetadata = len(itemmetadata[\"items\"])\n",
    "    for i in range(len_items_itemmetadata):\n",
    "        if (\"assemblyOptions\" in itemmetadata[\"items\"][i]):\n",
    "            del itemmetadata[\"items\"][i][\"assemblyOptions\"]\n",
    "    return itemmetadata\n",
    "    \n",
    "    \n",
    "def transform_ratesandbenefitsdata(data):\n",
    "    KEY_IDENTIFIERS = \"rateAndBenefitsIdentifiers\"\n",
    "    KEY_MATCH_PARAMS = \"matchedParameters\"\n",
    "    KEY_ADDINFO = \"additionalInfo\"\n",
    "    for i in range(len(data[KEY_IDENTIFIERS])):\n",
    "        if KEY_MATCH_PARAMS in data[KEY_IDENTIFIERS][i]:\n",
    "            del data[KEY_IDENTIFIERS][i][KEY_MATCH_PARAMS]\n",
    "        if KEY_ADDINFO in data[KEY_IDENTIFIERS][i]:\n",
    "            del data[KEY_IDENTIFIERS][i][KEY_ADDINFO]\n",
    "    return data\n",
    "\n",
    "\n",
    "def transform_customdata(customdata):\n",
    "    KEY_CUSTOMAPP = \"customApps\"\n",
    "    KEY_FIELDS = \"fields\"\n",
    "    KEY_EXTRA_CONTENT = \"cart-extra-context\"\n",
    "    if KEY_CUSTOMAPP in customdata:\n",
    "        for i in range(len(customdata[KEY_CUSTOMAPP])):\n",
    "            if KEY_FIELDS in customdata[KEY_CUSTOMAPP][i] and\\\n",
    "                KEY_EXTRA_CONTENT in customdata[KEY_CUSTOMAPP][i][KEY_FIELDS]:\n",
    "                    del customdata[KEY_CUSTOMAPP][i][KEY_FIELDS][KEY_EXTRA_CONTENT]\n",
    "    return customdata\n",
    "    \n",
    "def get_transform_data(field):\n",
    "    structuted_field = json.loads(field)\n",
    "    \n",
    "    if field == JSON_KEY_ITEMS:\n",
    "        structuted_field = parse_items(structuted_field)\n",
    "\n",
    "    if field == JSON_KEY_ITEMMETADATA:\n",
    "        structuted_field = transform_itemmetadata(structuted_field)\n",
    "\n",
    "    if field == JSON_KEY_RATESANDBDATA:\n",
    "        structuted_field = transform_ratesandbenefitsdata(structuted_field)\n",
    "\n",
    "    if field == JSON_KEY_CUSTOMDATA:\n",
    "        structuted_field = transform_customdata(structuted_field)\n",
    "    \n",
    "    if(type(structuted_field) == dict):\n",
    "        structuted_field = remove_attachments(structuted_field)\n",
    "    \n",
    "    return structuted_field\n",
    "    \n",
    "def transform_struct_df(df_no_structured, fields):\n",
    "    '''\n",
    "        Transforms to structured Data Frame. \n",
    "    '''\n",
    "    for field in fields:\n",
    "        fieldType = field.dataType\n",
    "        fieldName = field.name\n",
    "        \n",
    "        if(fieldType != StringType()):\n",
    "            udf_get_transform_data = UserDefinedFunction(get_transform_data, fieldType)\n",
    "            df_no_structured = df_no_structured.withColumn(fieldName, udf_get_transform_data(fieldName))\n",
    "    \n",
    "    return df_no_structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "u'Unsupported class file major version 55'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-9c159ad0acd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform_struct_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstructured_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Items\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/thaynan/anaconda2/envs/agora_vai/lib/python2.7/site-packages/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \"\"\"\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/thaynan/anaconda2/envs/agora_vai/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/thaynan/anaconda2/envs/agora_vai/lib/python2.7/site-packages/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: u'Unsupported class file major version 55'"
     ]
    }
   ],
   "source": [
    "df = transform_struct_df(df, structured_df.schema.fields)\n",
    "\n",
    "df.select(\"Items\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Particionando dados e escrevendo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getYear(lastChange, creationDate):\n",
    "    date = lastChange if lastChange is not None else creationDate\n",
    "    return date.split('T')[0].split('-')[0]\n",
    "\n",
    "def getMonth(lastChange, creationDate):\n",
    "    date = lastChange if lastChange is not None else creationDate\n",
    "    return date.split('T')[0].split('-')[1]\n",
    "\n",
    "def getDay(lastChange, creationDate):\n",
    "    date = lastChange if lastChange is not None else creationDate\n",
    "    return date.split('T')[0].split('-')[2]\n",
    "\n",
    "# Register functions as Spark UDFs \n",
    "udf_getYear = UserDefinedFunction(getYear, StringType())\n",
    "udf_getMonth = UserDefinedFunction(getMonth, StringType())\n",
    "udf_getDay = UserDefinedFunction(getDay, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "u'Unsupported class file major version 55'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-10e5dd611ad8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"YEAR\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# TODO: LastChange eh um campo nullable. Isso pode gerar problemas na hora de executar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#         as funções auxiliares anteriores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/thaynan/anaconda2/envs/agora_vai/lib/python2.7/site-packages/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \"\"\"\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/thaynan/anaconda2/envs/agora_vai/lib/python2.7/site-packages/py4j/java_gateway.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/thaynan/anaconda2/envs/agora_vai/lib/python2.7/site-packages/pyspark/sql/utils.pyc\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: u'Unsupported class file major version 55'"
     ]
    }
   ],
   "source": [
    "# Create the Columns for the Partitions\n",
    "df = df.withColumn('YEAR', udf_getYear(df.LastChange))\n",
    "df = df.withColumn('MONTH', udf_getMonth(df.LastChange))\n",
    "df = df.withColumn('DAY', udf_getDay(df.LastChange))\n",
    "\n",
    "\n",
    "df.select(\"YEAR\").show()\n",
    "# TODO: LastChange eh um campo nullable. Isso pode gerar problemas na hora de executar\n",
    "#         as funções auxiliares anteriores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save table to S3 using Parquet format and partitioning by defined columns\n",
    "df.write.partitionBy(['YEAR','MONTH','DAY','InstanceId'])\\\n",
    "    .mode('append')\\\n",
    "    .parquet('s3://vtex.datalake/consumable_tables/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
